{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16df7733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMain IDEA : Given a sequence of multimodal inputs, predict the final drone position + orientation.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main IDEA : Given a sequence of multimodal inputs, predict the final drone position + orientation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adffbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34dd1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "\n",
    "    csv_path: str\n",
    "    root_dir: str\n",
    "    out_dir: str = \"./checkpoints\"\n",
    "    \n",
    "    img_size: Tuple[int, int] = (224, 224)  # (H, W)\n",
    "    \n",
    "    seq_len: int = 5\n",
    "    seq_stride: int = 1\n",
    "    \n",
    "    batch_size: int = 8\n",
    "    epochs: int = 20\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-5\n",
    "    \n",
    "    num_workers: int = 4\n",
    "    pos_loss_weight: float = 1.0\n",
    "    ori_loss_weight: float = 100.0\n",
    "    \n",
    "    use_pretrained_backbones: bool = True\n",
    "    use_gru: bool = True\n",
    "    freeze_cnn: bool = False\n",
    "    \n",
    "    rgb_backbone: str = \"resnet18\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cc9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quaternion_normalize(q: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize quaternion to unit norm. q shape: (..., 4).\n",
    "    \"\"\"\n",
    "    return q / (q.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "\n",
    "def load_image(path: str, resize: Tuple[int, int], is_depth: bool) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Load an image from the given path, resize it to the specified dimensions,\n",
    "    \"\"\"\n",
    "    img = Image.open(path)   \n",
    "    \n",
    "    if is_depth:\n",
    "        # 2a. If it's a depth image → ensure it has an appropriate mode\n",
    "        # \"I;16\" = 16-bit grayscale, \"I\" = 32-bit int grayscale,\n",
    "        # \"F\" = 32-bit float grayscale, \"L\" = 8-bit grayscale\n",
    "        if img.mode not in (\"I;16\", \"I\", \"F\", \"L\"):\n",
    "            img = img.convert(\"I\")  # convert to 32-bit grayscale\n",
    "    else:\n",
    "        # 2b. If it's an RGB image → ensure mode is \"RGB\"\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "    \n",
    "    # 3. Resize the image to the given size (resize[0] = height, resize[1] = width)\n",
    "    # Use different interpolation:\n",
    "    # - RGB → bilinear (smooth)\n",
    "    # - Depth → nearest (preserves exact depth values)\n",
    "    img = img.resize((resize[1], resize[0]), Image.BILINEAR if not is_depth else Image.NEAREST)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9db293",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DronePoseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset: expects a CSV with modality + pose info.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: Config, split: str = \"train\", train_split: float = 0.9):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "\n",
    "        df = pd.read_csv(cfg.csv_path)\n",
    "\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "        self.df = df\n",
    "\n",
    "        n = len(df)\n",
    "        n_train = int(train_split * n)\n",
    "        \n",
    "        # split the dataset into train and validation sets\n",
    "        if split == \"train\":\n",
    "            self.df_split = df.iloc[:n_train].reset_index(drop=True)\n",
    "        else:\n",
    "            self.df_split = df.iloc[n_train:].reset_index(drop=True)\n",
    "\n",
    "        self.indices: List[int] = []\n",
    "\n",
    "        # The dataset works on sliding windows of length seq_len with stride seq_stride.\n",
    "        L = len(self.df_split)\n",
    "        for start in range(0, L - cfg.seq_len + 1, cfg.seq_stride):\n",
    "            self.indices.append(start)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Transforms:\n",
    "        - RGB → To tensor → Normalize with ImageNet stats.\n",
    "        - Depth → To tensor, but no normalization (since depth values are absolute, not colors).\n",
    "        \"\"\"\n",
    "        self.rgb_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.depth_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def _row_to_modalities(self, row: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Given a single row from the CSV, it loads all sensor modalities. \n",
    "\n",
    "        The topics are expected to be:\n",
    "        - RGB & Depth : rgb_file,depth_file,\n",
    "        - Sonar : sonar_front,sonar_back,sonar_left,sonar_right,\n",
    "        - IMU : imu_ang_vel_x,imu_ang_vel_y,imu_ang_vel_z,imu_lin_acc_x,imu_lin_acc_y,imu_lin_acc_z,\n",
    "        - GT : gt_pos_x,gt_pos_y,gt_pos_z,gt_orient_x,gt_orient_y,gt_orient_z,gt_orient_w\n",
    "        \"\"\"\n",
    "\n",
    "        rgb_path = os.path.join(self.cfg.root_dir, str(row[\"rgb_file\"]))\n",
    "        depth_path = os.path.join(self.cfg.root_dir, str(row[\"depth_file\"]))\n",
    "\n",
    "        rgb_img = load_image(rgb_path, self.cfg.img_size, is_depth=False)\n",
    "        depth_img = load_image(depth_path, self.cfg.img_size, is_depth=True)\n",
    "\n",
    "        rgb_tensor = self.rgb_transform(rgb_img)\n",
    "        depth_tensor = self.depth_transform(depth_img)\n",
    "\n",
    "        sonar = torch.tensor([row[\"sonar_front\"], row[\"sonar_back\"],\n",
    "                              row[\"sonar_left\"], row[\"sonar_right\"]], dtype=torch.float32)\n",
    "        \n",
    "        imu = torch.tensor([row[\"imu_ang_vel_x\"], row[\"imu_ang_vel_y\"], row[\"imu_ang_vel_z\"],\n",
    "                            row[\"imu_lin_acc_x\"], row[\"imu_lin_acc_y\"], row[\"imu_lin_acc_z\"]],\n",
    "                           dtype=torch.float32)\n",
    "\n",
    "        pos = torch.tensor([row[\"gt_pos_x\"], row[\"gt_pos_y\"], row[\"gt_pos_z\"]], dtype=torch.float32)\n",
    "        quat = torch.tensor([row[\"gt_orient_x\"], row[\"gt_orient_y\"],\n",
    "                             row[\"gt_orient_z\"], row[\"gt_orient_w\"]], dtype=torch.float32)\n",
    "\n",
    "        return {\"rgb\": rgb_tensor, \"depth\": depth_tensor, \"sonar\": sonar,\n",
    "                \"imu\": imu, \"pos\": pos, \"quat\": quat}\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        - Each index (idx) corresponds to a window of length seq_len starting at start.\n",
    "        - Instead of returning a pose per frame, it only takes the final frame's pose.\n",
    "        \"\"\"\n",
    "\n",
    "        start = self.indices[idx]\n",
    "        rows = self.df_split.iloc[start:start + self.cfg.seq_len]\n",
    "\n",
    "        rgbs, depths, sonars, imus = [], [], [], []\n",
    "        \n",
    "        for _, row in rows.iterrows():\n",
    "            \n",
    "            m = self._row_to_modalities(row)\n",
    "            rgbs.append(m[\"rgb\"])\n",
    "            depths.append(m[\"depth\"])\n",
    "            sonars.append(m[\"sonar\"])\n",
    "            imus.append(m[\"imu\"])\n",
    "\n",
    "        last = self._row_to_modalities(rows.iloc[-1]) # get only the pose of the last row \n",
    "\n",
    "        return {\n",
    "            \"rgb\": torch.stack(rgbs, dim=0),\n",
    "            \"depth\": torch.stack(depths, dim=0),\n",
    "            \"sonar\": torch.stack(sonars, dim=0),\n",
    "            \"imu\": torch.stack(imus, dim=0),\n",
    "            \"pos\": last[\"pos\"],\n",
    "            \"quat\": last[\"quat\"],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USAGE :\n",
    "1. Depth Processing:\n",
    "depth_model = SmallDepthCNN(out_dim=128)\n",
    "depth_feats = depth_model(depth_tensor)  # depth_tensor: (B,T,1,H,W)\n",
    "\n",
    "2. RGB Processing:\n",
    "rgb_model = make_resnet_feature_extractor(\"resnet18\", pretrained=True, out_dim=256)\n",
    "rgb_feats = rgb_model(rgb_tensor)  # rgb_tensor: (B,T,3,H,W)\n",
    "\"\"\"\n",
    "\n",
    "class SmallDepthCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: depth image → shape (B, 1, H, W) or (B, T, 1, H, W) if a sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, out_dim: int = 128):\n",
    "\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, stride=2, padding=2), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "        )\n",
    "        \n",
    "        # Flatten → (B, 128) → fully connected layer → (B, out_dim).\n",
    "        self.fc = nn.Linear(128, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Handles both single frame (B,1,H,W) and sequence (B,T,1,H,W).\n",
    "        - (B, out_dim) if single frame.\n",
    "        - (B, T, out_dim) if sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        is_seq = (x.dim() == 5)\n",
    "\n",
    "        if is_seq:\n",
    "            B, T = x.shape[:2]\n",
    "            x = x.view(B*T, *x.shape[2:])\n",
    "        feat = self.net(x).flatten(1)\n",
    "        feat = self.fc(feat)\n",
    "\n",
    "        if is_seq:\n",
    "            feat = feat.view(B, T, -1)\n",
    "\n",
    "        return feat\n",
    "\n",
    "\n",
    "def make_resnet_feature_extractor(backbone=\"resnet18\", pretrained=True, out_dim=256):\n",
    "    \"\"\"\n",
    "    Creates a feature extractor for RGB images using a ResNet backbone (18/34/50).\n",
    "    \"\"\"\n",
    "    if backbone == \"resnet18\":\n",
    "        m = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        feat_dim = 512\n",
    "\n",
    "    elif backbone == \"resnet34\":\n",
    "        m = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
    "        feat_dim = 512\n",
    "\n",
    "    elif backbone == \"resnet50\":\n",
    "        m = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "        feat_dim = 2048\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "\n",
    "    body = nn.Sequential(*list(m.children())[:-1]) # Reduces ResNet features to desired out_dim (e.g., 256). \n",
    "    proj = nn.Linear(feat_dim, out_dim)\n",
    "\n",
    "    class ResnetFeat(nn.Module):\n",
    "        \"\"\"\n",
    "        (B,T,C,H,W) → flattened into (B*T, C,H,W) → pass through ResNet → projected features → reshape back to (B,T,out_dim)\n",
    "        \"\"\"\n",
    "        def __init__(self, body, proj):\n",
    "            super().__init__()\n",
    "            self.body = body\n",
    "            self.proj = proj\n",
    "\n",
    "        def forward(self, x):\n",
    "            is_seq = (x.dim() == 5)\n",
    "            if is_seq:\n",
    "                B, T = x.shape[:2]\n",
    "                x = x.view(B*T, *x.shape[2:])\n",
    "            f = self.body(x).flatten(1)\n",
    "            f = self.proj(f)\n",
    "            if is_seq:\n",
    "                f = f.view(B, T, -1)\n",
    "            return f\n",
    "\n",
    "    return ResnetFeat(body, proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9016b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseNetMultiModal(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Config, rgb_feat_dim=256, depth_feat_dim=128,\n",
    "                 sonar_dim=32, imu_dim=64, fused_dim=256, rnn_hidden=256):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.rgb_encoder = make_resnet_feature_extractor(cfg.rgb_backbone,\n",
    "                                                         cfg.use_pretrained_backbones,\n",
    "                                                         rgb_feat_dim)\n",
    "        \n",
    "        self.depth_encoder = SmallDepthCNN(out_dim = depth_feat_dim)\n",
    "\n",
    "        self.sonar_mlp = nn.Sequential(nn.Linear(4, 32), nn.ReLU(),\n",
    "                                       nn.Linear(32, sonar_dim), nn.ReLU())\n",
    "        \n",
    "        self.imu_mlp = nn.Sequential(nn.Linear(6, 64), nn.ReLU(),\n",
    "                                     nn.Linear(64, imu_dim), nn.ReLU())\n",
    "\n",
    "        in_fuse = rgb_feat_dim + depth_feat_dim + sonar_dim + imu_dim # this is the main stacking of all multi-modal features\n",
    "\n",
    "        self.fuse = nn.Sequential(nn.Linear(in_fuse, fused_dim), nn.ReLU(),\n",
    "                                  nn.Linear(fused_dim, fused_dim), nn.ReLU())\n",
    "\n",
    "        self.use_gru = cfg.use_gru\n",
    "\n",
    "        if self.use_gru: # GRU takes fused features over a sequence → hidden size rnn_hidden.\n",
    "            self.rnn = nn.GRU(fused_dim, rnn_hidden, batch_first=True)\n",
    "            head_in = rnn_hidden\n",
    "        else:\n",
    "            head_in = fused_dim\n",
    "\n",
    "        # final head to predict position + orientation\n",
    "        self.head = nn.Sequential(nn.Linear(head_in, 128), nn.ReLU(),\n",
    "                                  nn.Linear(128, 7))\n",
    "\n",
    "    def forward(self, rgb, depth, sonar, imu):\n",
    "\n",
    "        rgb_f = self.rgb_encoder(rgb)\n",
    "        depth_f = self.depth_encoder(depth)\n",
    "        sonar_f = self.sonar_mlp(sonar)\n",
    "        imu_f = self.imu_mlp(imu)\n",
    "\n",
    "        fused = torch.cat([rgb_f, depth_f, sonar_f, imu_f], dim=-1)\n",
    "        fused = self.fuse(fused)\n",
    "\n",
    "        if self.use_gru:\n",
    "            out, _ = self.rnn(fused)\n",
    "            feat_T = out[:, -1, :]\n",
    "\n",
    "        else:\n",
    "            feat_T = fused[:, -1, :]\n",
    "\n",
    "        pose = self.head(feat_T)\n",
    "\n",
    "        pos, quat = pose[:, :3], quaternion_normalize(pose[:, 3:])\n",
    "        \n",
    "        return pos, quat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5fe6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, pos_w=1.0, ori_w=100.0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.pos_w = pos_w\n",
    "        self.ori_w = ori_w\n",
    "\n",
    "    def forward(self, pos_pred, quat_pred, pos_gt, quat_gt):\n",
    "\n",
    "        pos_loss = F.mse_loss(pos_pred, pos_gt)\n",
    "        quat_gt = quaternion_normalize(quat_gt)\n",
    "        ori_loss = F.mse_loss(quat_pred, quat_gt)\n",
    "        \n",
    "        return self.pos_w * pos_loss + self.ori_w * ori_loss, pos_loss, ori_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51a795a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Purpose: Convert a list of samples from the dataset into a batched tensor dictionary.\n",
    "    \"\"\"\n",
    "    return {k: torch.stack([b[k] for b in batch], dim=0) for k in batch[0]}\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loss_fn, optimizer, loader, device):\n",
    "    \"\"\"\n",
    "    Sets the model to training mode (activates dropout, batchnorm, etc.)\n",
    "    and prints position, orientation, and combined loss per batch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total, pos_total, ori_total = 0, 0, 0\n",
    "\n",
    "    for i, batch in enumerate(loader, 1):\n",
    "        rgb, depth, sonar, imu = (batch[\"rgb\"].to(device), batch[\"depth\"].to(device),\n",
    "                                  batch[\"sonar\"].to(device), batch[\"imu\"].to(device))\n",
    "        \n",
    "        pos_gt, quat_gt = batch[\"pos\"].to(device), batch[\"quat\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pos_pred, quat_pred = model(rgb, depth, sonar, imu)\n",
    "        loss, pos_l, ori_l = loss_fn(pos_pred, quat_pred, pos_gt, quat_gt)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item()\n",
    "        pos_total += pos_l.item()\n",
    "        ori_total += ori_l.item()\n",
    "\n",
    "        # Print losses for this batch\n",
    "        # print(f\"Batch {i}: Pos Loss = {pos_l.item():.4f}, Ori Loss = {ori_l.item():.4f}, Total Loss = {loss.item():.4f}\")\n",
    "        \n",
    "    return total/len(loader), pos_total/len(loader), ori_total/len(loader)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, loss_fn, loader, device):\n",
    "    \"\"\"\n",
    "    - model.eval() → disables dropout, batchnorm in training mode.\n",
    "    - torch.no_grad() → prevents gradient computation (saves memory).\n",
    "    \n",
    "    Rest is same as train_one_epoch, but without optimizer step.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total, pos_total, ori_total = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "\n",
    "            rgb, depth, sonar, imu = (batch[\"rgb\"].to(device), batch[\"depth\"].to(device),\n",
    "                                      batch[\"sonar\"].to(device), batch[\"imu\"].to(device))\n",
    "            pos_gt, quat_gt = batch[\"pos\"].to(device), batch[\"quat\"].to(device)\n",
    "\n",
    "            pos_pred, quat_pred = model(rgb, depth, sonar, imu)\n",
    "            loss, pos_l, ori_l = loss_fn(pos_pred, quat_pred, pos_gt, quat_gt)\n",
    "\n",
    "            total += loss.item(); pos_total += pos_l.item(); ori_total += ori_l.item()\n",
    "\n",
    "    return total/len(loader), pos_total/len(loader), ori_total/len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d7a7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(cfg: Config):\n",
    "\n",
    "    os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "    train_set = DronePoseDataset(cfg, \"train\")\n",
    "    val_set = DronePoseDataset(cfg, \"val\")\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True,\n",
    "                              num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "    \n",
    "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False,\n",
    "                            num_workers=cfg.num_workers, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "    model = PoseNetMultiModal(cfg).to(cfg.device)\n",
    "\n",
    "    if cfg.freeze_cnn:\n",
    "        for p in model.rgb_encoder.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    loss_fn = PoseLoss(cfg.pos_loss_weight, cfg.ori_loss_weight)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                  lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                           factor=0.5, patience=3)\n",
    "\n",
    "    best_val = math.inf\n",
    "    \n",
    "    for epoch in range(1, cfg.epochs+1):\n",
    "        tr_loss, tr_pos, tr_ori = train_one_epoch(model, loss_fn, optimizer, train_loader, cfg.device)\n",
    "        va_loss, va_pos, va_ori = evaluate(model, loss_fn, val_loader, cfg.device)\n",
    "        scheduler.step(va_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train {tr_loss:.4f} (pos {tr_pos:.4f}, ori {tr_ori:.4f}) \"\n",
    "              f\"| Val {va_loss:.4f} (pos {va_pos:.4f}, ori {va_ori:.4f})\")\n",
    "\n",
    "        ckpt = {'epoch': epoch, 'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(), 'cfg': cfg.__dict__}\n",
    "        torch.save(ckpt, os.path.join(cfg.out_dir, 'last.pth'))\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            \n",
    "            best_val = va_loss\n",
    "            torch.save({**ckpt, 'val_loss': va_loss}, os.path.join(cfg.out_dir, 'best.pth'))\n",
    "            print(f\"  → New best model (val {va_loss:.4f}) saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8768f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose_estimate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
